{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import typing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "# Load data from Crunch\n",
    "import crunch\n",
    "\n",
    "crunch = crunch.load_notebook()\n",
    "\n",
    "class CausalDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for causal modeling.\"\"\"\n",
    "    def __init__(self, X: typing.List[pd.DataFrame], y: typing.List[pd.DataFrame]) -> None:\n",
    "        self.X = np.zeros([len(X), 1000, 10], dtype=np.float32)\n",
    "        self.y = np.zeros([len(X), 10, 10], dtype=np.float32)\n",
    "        self.target_mask = np.zeros([len(X), 10, 10], dtype=bool)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            self.X[i, :X[i].shape[0], :X[i].shape[1]] = X[i].values\n",
    "            self.y[i, :y[i].shape[0], :y[i].shape[1]] = y[i].values\n",
    "            self.target_mask[i, :y[i].shape[0], :y[i].shape[1]] = True\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        return {\n",
    "            'X': self.X[idx],\n",
    "            'y': self.y[idx],\n",
    "            'target_mask': self.target_mask[idx]\n",
    "        }\n",
    "\n",
    "def preprocessing(X: pd.DataFrame) -> torch.Tensor:\n",
    "    \"\"\"Preprocess input DataFrame for the model.\"\"\"\n",
    "    return torch.Tensor(X.values).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalModel(nn.Module):\n",
    "    \"\"\"Causal model architecture using feedforward neural networks.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=64):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 2 * d_model)\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        q, k = self.input_layer(x.unsqueeze(-1)).chunk(2, dim=-1)\n",
    "        x = torch.einsum('b s i d, b s j d -> b i j d', q, k) * (x.shape[1] ** -0.5)\n",
    "        return self.final(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ModelWrapper(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning wrapper for the causal model.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=64, lr=1e-3, pos_weight=5.0):\n",
    "        super().__init__()\n",
    "        self.model = CausalModel(d_model)\n",
    "        self.train_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, train_batch: dict, batch_idx: int):\n",
    "        x = train_batch['X']\n",
    "        y = train_batch['y']\n",
    "        target_mask = train_batch['target_mask']\n",
    "        preds = self(x)\n",
    "        loss = self.train_criterion(preds[target_mask], y[target_mask])\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch: dict, batch_idx: int):\n",
    "        x = val_batch['X']\n",
    "        y = val_batch['y']\n",
    "        target_mask = val_batch['target_mask']\n",
    "        preds = self(x)\n",
    "        loss = self.train_criterion(preds[target_mask], y[target_mask])\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "\n",
    "def transform_proba_to_DAG(nodes: typing.List[str], pred: np.ndarray) -> np.ndarray:\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    x_index, y_index = np.unravel_index(np.argsort(pred.ravel())[::-1], pred.shape)\n",
    "    for i, j in zip(x_index, y_index):\n",
    "        n1 = nodes[i]\n",
    "        n2 = nodes[j]\n",
    "        if i == j or (n1 == 'X' and n2 == 'Y') or (n1 == 'Y' and n2 == 'X'):\n",
    "            continue\n",
    "        if pred[i, j] > 0.5:\n",
    "            G.add_edge(n1, n2)\n",
    "            if not nx.is_directed_acyclic_graph(G):\n",
    "                G.remove_edge(n1, n2)\n",
    "    return nx.to_numpy_array(G)\n",
    "\n",
    "def create_graph_label() -> typing.Tuple[dict, dict]:\n",
    "    graph_label = {\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]): \"Cause of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]): \"Cause of Y\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]): \"Consequence of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]): \"Consequence of Y\",\n",
    "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}): \"Independent\",\n",
    "    }\n",
    "    nodelist = [\"v\", \"X\", \"Y\"]\n",
    "    adjacency_label = {\n",
    "        graph_nodes_representation(graph, nodelist): label  # noqa: F821\n",
    "        for graph, label in graph_label.items()\n",
    "    }\n",
    "    return graph_label, adjacency_label\n",
    "\n",
    "def get_labels(adjacency_matrix: pd.DataFrame, adjacency_label: dict) -> dict:\n",
    "    result = {}\n",
    "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
    "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]\n",
    "        key = tuple(submatrix.values.flatten())\n",
    "        result[variable] = adjacency_label[key]\n",
    "    return result\n",
    "\n",
    "def train(\n",
    "    X_train: typing.Dict[str, pd.DataFrame],\n",
    "    y_train: typing.Dict[str, pd.DataFrame],\n",
    "    model_directory_path: str,\n",
    "    batch_size: int = 64,\n",
    "    max_epochs: int = 10,\n",
    "    learning_rate: float = 1e-3,\n",
    ") -> None:\n",
    "    X = [X_train[dataset_id] for dataset_id in X_train]\n",
    "    y = [y_train[dataset_id] for dataset_id in y_train]\n",
    "    dataset = CausalDataset(X, y)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    model = ModelWrapper(lr=learning_rate)\n",
    "    trainer = pl.Trainer(accelerator=\"auto\", max_epochs=max_epochs, logger=False)\n",
    "    trainer.fit(model, train_dataloader)\n",
    "\n",
    "    model_path_file = os.path.join(model_directory_path, \"model.pt\")\n",
    "    torch.save(model.state_dict(), model_path_file)\n",
    "\n",
    "def infer(\n",
    "    X_test: typing.Dict[str, pd.DataFrame],\n",
    "    model_directory_path: str,\n",
    "    id_column_name: str,\n",
    "    prediction_column_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    model_path_file = os.path.join(model_directory_path, \"model.pt\")\n",
    "    model = CausalModel(d_model=64)\n",
    "    model.load_state_dict(torch.load(model_path_file, map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "    submission_file = {}\n",
    "    for name in X_test:\n",
    "        X = X_test[name]\n",
    "        x = preprocessing(X)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)[0]\n",
    "            pred = torch.sigmoid(pred)\n",
    "            pred = pred.cpu().numpy()\n",
    "\n",
    "        nodes = list(X.columns)\n",
    "        pred = transform_proba_to_DAG(nodes, pred).astype(int)\n",
    "        G = pd.DataFrame(pred, columns=nodes, index=nodes)\n",
    "\n",
    "        for i in nodes:\n",
    "            for j in nodes:\n",
    "                submission_file[f'{name}_{i}_{j}'] = int(G.loc[i, j])\n",
    "\n",
    "    submission_file = pd.Series(submission_file).reset_index()\n",
    "    submission_file.columns = [id_column_name, prediction_column_name]\n",
    "    crunch.test(\n",
    "    no_determinism_check=True\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
